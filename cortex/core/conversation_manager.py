"""
Conversation Manager - Gestion intelligente de l'historique avec r√©sum√©s

Syst√®me de r√©sum√© hi√©rarchique:
- Sections de 5000 tokens ‚Üí r√©sum√© automatique de section
- Quand toutes sections atteignent 10k tokens ‚Üí r√©sum√© global + nouvelle section
- Garde trace des nouveaux messages non-r√©sum√©s
"""

import json
import tiktoken
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict

from cortex.core.llm_client import LLMClient
from cortex.core.model_router import ModelTier


@dataclass
class ConversationSection:
    """Une section de conversation avec son r√©sum√©"""
    id: str
    messages: List[Dict[str, str]]  # Messages bruts
    summary: Optional[str]  # R√©sum√© de cette section (None si pas encore r√©sum√©)
    token_count: int  # Nombre de tokens dans cette section
    created_at: str
    summarized_at: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dict pour JSON"""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ConversationSection':
        """Cr√©e une section depuis un dict"""
        return cls(**data)


class ConversationManager:
    """
    Gestionnaire d'historique de conversation avec r√©sum√©s automatiques

    Architecture:
    - Sections de ~5000 tokens (nouvelle conversation active)
    - Quand section atteint 5000 tokens ‚Üí r√©sum√© + nouvelle section
    - Quand toutes sections totalisent 10k tokens ‚Üí r√©sum√© global
    """

    SECTION_TOKEN_THRESHOLD = 5000  # Seuil pour r√©sumer une section
    GLOBAL_TOKEN_THRESHOLD = 10000  # Seuil pour r√©sum√© global

    def __init__(
        self,
        llm_client: LLMClient,
        storage_path: str = "cortex/data/conversation_history.json"
    ):
        """
        Initialize Conversation Manager

        Args:
            llm_client: Client LLM pour g√©n√©rer les r√©sum√©s
            storage_path: Chemin du fichier JSON de stockage
        """
        self.llm_client = llm_client
        self.storage_path = Path(storage_path)
        self.sections: List[ConversationSection] = []
        self.current_section: ConversationSection = self._create_new_section()
        self.global_summary: Optional[str] = None

        # Encoder tiktoken pour compter les tokens
        try:
            self.encoder = tiktoken.get_encoding("cl100k_base")
        except Exception:
            self.encoder = None

        self._load()

    def _create_new_section(self) -> ConversationSection:
        """Cr√©e une nouvelle section vide"""
        section_id = f"section_{len(self.sections) + 1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        return ConversationSection(
            id=section_id,
            messages=[],
            summary=None,
            token_count=0,
            created_at=datetime.now().isoformat()
        )

    def _count_tokens(self, text: str) -> int:
        """Compte le nombre de tokens dans un texte"""
        if self.encoder:
            return len(self.encoder.encode(text))
        else:
            # Fallback: approximation (1 token ‚âà 4 caract√®res)
            return len(text) // 4

    def _count_messages_tokens(self, messages: List[Dict[str, str]]) -> int:
        """Compte les tokens dans une liste de messages"""
        total = 0
        for msg in messages:
            content = msg.get('content', '')
            total += self._count_tokens(content)
        return total

    def _load(self):
        """Charge l'historique depuis le fichier"""
        if self.storage_path.exists():
            try:
                with open(self.storage_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.sections = [
                        ConversationSection.from_dict(s)
                        for s in data.get('sections', [])
                    ]
                    self.global_summary = data.get('global_summary')

                    # Charger la section courante
                    current_data = data.get('current_section')
                    if current_data:
                        self.current_section = ConversationSection.from_dict(current_data)
                    else:
                        self.current_section = self._create_new_section()
            except Exception as e:
                print(f"Warning: Failed to load conversation history: {e}")
                self.sections = []
                self.current_section = self._create_new_section()
        else:
            # Cr√©er le r√©pertoire si n√©cessaire
            self.storage_path.parent.mkdir(parents=True, exist_ok=True)

    def _save(self):
        """Sauvegarde l'historique dans le fichier"""
        try:
            data = {
                'sections': [section.to_dict() for section in self.sections],
                'current_section': self.current_section.to_dict(),
                'global_summary': self.global_summary,
                'last_updated': datetime.now().isoformat()
            }
            with open(self.storage_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"Error: Failed to save conversation history: {e}")

    def add_message(self, role: str, content: str):
        """
        Ajoute un message √† la conversation
        D√©clenche automatiquement les r√©sum√©s si n√©cessaire

        Args:
            role: "user", "assistant", ou "system"
            content: Contenu du message
        """
        message = {"role": role, "content": content}
        self.current_section.messages.append(message)

        # Mettre √† jour le compteur de tokens
        self.current_section.token_count = self._count_messages_tokens(
            self.current_section.messages
        )

        # V√©rifier si on doit r√©sumer la section courante
        if self.current_section.token_count >= self.SECTION_TOKEN_THRESHOLD:
            self._summarize_current_section()

        # V√©rifier si on doit faire un r√©sum√© global
        total_tokens = self._get_total_tokens()
        if total_tokens >= self.GLOBAL_TOKEN_THRESHOLD:
            self._create_global_summary()

        self._save()

    def _summarize_current_section(self):
        """R√©sume la section courante et cr√©e une nouvelle section"""
        if not self.current_section.messages:
            return

        print("\nüîÑ R√©sum√© automatique de la section en cours...")

        # Cr√©er un prompt de r√©sum√©
        messages_text = "\n\n".join([
            f"{msg['role'].upper()}: {msg['content']}"
            for msg in self.current_section.messages
        ])

        summary_prompt = f"""R√©sume cette section de conversation de mani√®re concise mais compl√®te.
Garde tous les d√©tails techniques importants, les d√©cisions prises, et le contexte n√©cessaire pour continuer la conversation.

CONVERSATION:
{messages_text}

R√âSUM√â (format structur√©):"""

        # Utiliser DeepSeek pour le r√©sum√© (√©conomique)
        try:
            response = self.llm_client.complete(
                messages=[{"role": "user", "content": summary_prompt}],
                tier=ModelTier.DEEPSEEK,
                max_tokens=1000,
                temperature=0.3
            )

            self.current_section.summary = response.content
            self.current_section.summarized_at = datetime.now().isoformat()

            # Archiver la section
            self.sections.append(self.current_section)

            # Cr√©er une nouvelle section
            self.current_section = self._create_new_section()

            print(f"‚úì Section r√©sum√©e ({len(self.sections)} sections totales)")

        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors du r√©sum√© de section: {e}")

    def _create_global_summary(self):
        """Cr√©e un r√©sum√© global et r√©initialise les sections"""
        if not self.sections:
            return

        print("\nüîÑ R√©sum√© global de toutes les sections...")

        # Collecter tous les r√©sum√©s de sections
        section_summaries = []
        for section in self.sections:
            if section.summary:
                section_summaries.append(f"Section {section.id}:\n{section.summary}")

        # Ajouter la section courante si elle a du contenu
        if self.current_section.messages:
            messages_text = "\n".join([
                f"{msg['role']}: {msg['content'][:200]}..."
                for msg in self.current_section.messages
            ])
            section_summaries.append(f"Section courante (non r√©sum√©e):\n{messages_text}")

        all_summaries = "\n\n---\n\n".join(section_summaries)

        global_prompt = f"""Cr√©e un r√©sum√© global ultra-condens√© de toute cette conversation.
Ce r√©sum√© sera le contexte de base pour les futures conversations.
Inclus UNIQUEMENT les informations essentielles et critiques.

SECTIONS:
{all_summaries}

R√âSUM√â GLOBAL (maximum 500 mots):"""

        # Utiliser DeepSeek pour le r√©sum√© global
        try:
            response = self.llm_client.complete(
                messages=[{"role": "user", "content": global_prompt}],
                tier=ModelTier.DEEPSEEK,
                max_tokens=800,
                temperature=0.3
            )

            # Combiner avec le r√©sum√© global pr√©c√©dent si existe
            if self.global_summary:
                self.global_summary = f"{self.global_summary}\n\n---NOUVEAU CYCLE---\n\n{response.content}"
            else:
                self.global_summary = response.content

            # R√©initialiser les sections (on garde que le r√©sum√© global)
            self.sections = []

            print(f"‚úì R√©sum√© global cr√©√©, sections r√©initialis√©es")

        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors du r√©sum√© global: {e}")

    def _get_total_tokens(self) -> int:
        """Calcule le nombre total de tokens de toutes les sections"""
        total = sum(section.token_count for section in self.sections)
        total += self.current_section.token_count
        return total

    def get_context_for_llm(self, max_tokens: int = 4000) -> List[Dict[str, str]]:
        """
        Construit le contexte optimal pour un appel LLM

        Priorit√©:
        1. R√©sum√© global (si existe)
        2. R√©sum√©s des sections archiv√©es
        3. Messages bruts de la section courante

        Args:
            max_tokens: Limite de tokens pour le contexte

        Returns:
            Liste de messages optimis√©e
        """
        context_messages = []
        token_budget = max_tokens

        # 1. Ajouter le r√©sum√© global si existe
        if self.global_summary:
            summary_msg = {
                "role": "system",
                "content": f"CONTEXTE GLOBAL PR√âC√âDENT:\n{self.global_summary}"
            }
            tokens = self._count_tokens(summary_msg['content'])
            if tokens < token_budget:
                context_messages.append(summary_msg)
                token_budget -= tokens

        # 2. Ajouter les r√©sum√©s de sections (les plus r√©centes en premier)
        for section in reversed(self.sections[-5:]):  # Max 5 derni√®res sections
            if section.summary and token_budget > 200:
                section_msg = {
                    "role": "system",
                    "content": f"SECTION PR√âC√âDENTE:\n{section.summary}"
                }
                tokens = self._count_tokens(section_msg['content'])
                if tokens < token_budget:
                    context_messages.insert(1 if self.global_summary else 0, section_msg)
                    token_budget -= tokens

        # 3. Ajouter les messages bruts de la section courante
        for msg in self.current_section.messages:
            tokens = self._count_tokens(msg['content'])
            if tokens < token_budget:
                context_messages.append(msg)
                token_budget -= tokens
            else:
                break

        return context_messages

    def get_statistics(self) -> Dict[str, Any]:
        """Obtient des statistiques sur l'historique"""
        return {
            'total_sections': len(self.sections),
            'current_section_messages': len(self.current_section.messages),
            'current_section_tokens': self.current_section.token_count,
            'total_tokens': self._get_total_tokens(),
            'has_global_summary': self.global_summary is not None,
            'section_threshold': self.SECTION_TOKEN_THRESHOLD,
            'global_threshold': self.GLOBAL_TOKEN_THRESHOLD
        }

    def clear_history(self):
        """Efface tout l'historique"""
        self.sections = []
        self.current_section = self._create_new_section()
        self.global_summary = None
        self._save()


def create_conversation_manager(
    llm_client: LLMClient,
    storage_path: str = "cortex/data/conversation_history.json"
) -> ConversationManager:
    """Factory function pour cr√©er un ConversationManager"""
    return ConversationManager(llm_client, storage_path)


# Test si ex√©cut√© directement
if __name__ == "__main__":
    from cortex.core.llm_client import LLMClient

    print("Testing Conversation Manager...")

    client = LLMClient()
    manager = ConversationManager(client, "cortex/data/test_conversation.json")

    # Test: Ajouter des messages
    print("\n1. Adding messages...")
    manager.add_message("user", "Hello, I need help with authentication")
    manager.add_message("assistant", "I can help you with that. What authentication method?")

    # Stats
    stats = manager.get_statistics()
    print(f"Stats: {stats}")

    print("\n‚úì Conversation Manager works correctly!")

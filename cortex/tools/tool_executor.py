"""
Tool Executor - Ex√©cute automatiquement les tools demand√©s par le LLM
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from cortex.tools.standard_tool import StandardTool
from cortex.core.llm_client import LLMClient, LLMResponse, ModelTier, ToolCall


@dataclass
class ExecutionResult:
    """R√©sultat de l'ex√©cution d'un tool"""
    tool_name: str
    tool_call_id: str
    success: bool
    result: Any
    error: Optional[str] = None


class ToolExecutor:
    """
    Ex√©cuteur de tools avec support du tool calling natif

    Workflow:
    1. Envoyer requ√™te au LLM avec tools disponibles
    2. Si LLM demande un tool, l'ex√©cuter
    3. Renvoyer le r√©sultat au LLM
    4. R√©p√©ter jusqu'√† r√©ponse finale
    """

    def __init__(self, llm_client: Optional[LLMClient] = None):
        self.llm_client = llm_client or LLMClient()
        self.tools: Dict[str, StandardTool] = {}
        self.max_iterations = 10  # Protection contre boucles infinies

    def register_tool(self, tool: StandardTool):
        """Enregistre un tool disponible"""
        self.tools[tool.name] = tool

    def register_tools(self, tools: List[StandardTool]):
        """Enregistre plusieurs tools"""
        for tool in tools:
            self.register_tool(tool)

    def execute_with_tools(
        self,
        messages: List[Dict[str, str]],
        tier: ModelTier = ModelTier.DEEPSEEK,
        max_tokens: int = 2048,
        temperature: float = 1.0,
        tools: Optional[List[StandardTool]] = None,
        verbose: bool = False
    ) -> LLMResponse:
        """
        Ex√©cute une requ√™te avec support automatique des tools

        Args:
            messages: Messages de conversation
            tier: Tier du mod√®le LLM
            max_tokens: Tokens max
            temperature: Temp√©rature
            tools: Tools √† utiliser (si None, utilise tous les tools enregistr√©s)
            verbose: Afficher les √©tapes interm√©diaires

        Returns:
            R√©ponse finale du LLM
        """
        # Utiliser les tools fournis ou ceux enregistr√©s
        available_tools = tools or list(self.tools.values())

        if not available_tools:
            # Pas de tools, appel direct
            return self.llm_client.complete(
                messages=messages,
                tier=tier,
                max_tokens=max_tokens,
                temperature=temperature
            )

        # Conversation avec tools
        conversation_messages = messages.copy()
        iteration = 0

        while iteration < self.max_iterations:
            iteration += 1

            if verbose:
                print(f"\n[Iteration {iteration}] Calling LLM...")

            # Appeler le LLM avec les tools
            response = self.llm_client.complete(
                messages=conversation_messages,
                tier=tier,
                max_tokens=max_tokens,
                temperature=temperature,
                tools=available_tools
            )

            # Si pas de tool calls, c'est la r√©ponse finale
            if not response.tool_calls:
                if verbose:
                    print(f"[Iteration {iteration}] Final response received")
                return response

            # Ex√©cuter les tools demand√©s
            if verbose:
                print(f"[Iteration {iteration}] Executing {len(response.tool_calls)} tool(s)...")

            tool_results = []
            for tool_call in response.tool_calls:
                result = self._execute_tool_call(tool_call, verbose)
                tool_results.append(result)

            # Ajouter les r√©sultats √† la conversation
            # Format diff√©rent selon le provider
            if tier == ModelTier.CLAUDE:
                # Format Anthropic
                conversation_messages.append({
                    "role": "assistant",
                    "content": [{
                        "type": "tool_use",
                        "id": tc.id,
                        "name": tc.name,
                        "input": tc.arguments
                    } for tc in response.tool_calls]
                })

                conversation_messages.append({
                    "role": "user",
                    "content": [{
                        "type": "tool_result",
                        "tool_use_id": result.tool_call_id,
                        "content": str(result.result) if result.success else result.error
                    } for result, tc in zip(tool_results, response.tool_calls)]
                })
            else:
                # Format OpenAI (Nano et DeepSeek)
                conversation_messages.append({
                    "role": "assistant",
                    "content": None,
                    "tool_calls": [{
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.name,
                            "arguments": str(tc.arguments)
                        }
                    } for tc in response.tool_calls]
                })

                for result, tc in zip(tool_results, response.tool_calls):
                    conversation_messages.append({
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": str(result.result) if result.success else result.error
                    })

        # Si on arrive ici, on a d√©pass√© max_iterations
        raise RuntimeError(f"Max iterations ({self.max_iterations}) reached")

    def _execute_tool_call(self, tool_call: ToolCall, verbose: bool = False) -> ExecutionResult:
        """Ex√©cute un tool call"""
        tool_name = tool_call.name

        print(f"  üîß Executing tool: {tool_name}")
        if verbose:
            print(f"     Arguments: {tool_call.arguments}")

        # V√©rifier que le tool existe
        if tool_name not in self.tools:
            error_msg = f"Tool '{tool_name}' not found"
            print(f"    ‚ùå ERROR: {error_msg}")
            return ExecutionResult(
                tool_name=tool_name,
                tool_call_id=tool_call.id,
                success=False,
                result=None,
                error=error_msg
            )

        tool = self.tools[tool_name]

        # Valider les param√®tres
        valid, error = tool.validate_params(tool_call.arguments)
        if not valid:
            print(f"    ‚ùå VALIDATION ERROR: {error}")
            return ExecutionResult(
                tool_name=tool_name,
                tool_call_id=tool_call.id,
                success=False,
                result=None,
                error=error
            )

        # Ex√©cuter le tool
        try:
            result = tool.execute(**tool_call.arguments)

            # V√©rifier si c'est un succ√®s ou un √©chec
            is_success = True
            if isinstance(result, dict):
                is_success = result.get("success", True)

            if is_success:
                print(f"    ‚úÖ SUCCESS: {self._format_result(result)}")
            else:
                print(f"    ‚ö†Ô∏è  PARTIAL SUCCESS: {self._format_result(result)}")

            return ExecutionResult(
                tool_name=tool_name,
                tool_call_id=tool_call.id,
                success=True,
                result=result
            )
        except Exception as e:
            error_msg = f"Execution error: {str(e)}"
            print(f"    ‚ùå EXECUTION ERROR: {error_msg}")

            return ExecutionResult(
                tool_name=tool_name,
                tool_call_id=tool_call.id,
                success=False,
                result=None,
                error=error_msg
            )

    def _format_result(self, result: Any) -> str:
        """Formate le r√©sultat pour affichage"""
        if isinstance(result, dict):
            if "success" in result:
                if result["success"]:
                    data = result.get("data", result)
                    if isinstance(data, str) and len(data) > 100:
                        return f"{str(data)[:100]}..."
                    return str(data)
                else:
                    return f"Error: {result.get('error', 'Unknown error')}"
            return str(result)[:100]
        elif isinstance(result, str):
            return result[:100] if len(result) > 100 else result
        else:
            return str(result)[:100]


# Exemple d'utilisation
if __name__ == "__main__":
    from cortex.tools.standard_tool import tool

    # Cr√©er quelques tools de test
    @tool(
        name="add",
        description="Add two numbers together",
        parameters={
            "type": "object",
            "properties": {
                "a": {"type": "number", "description": "First number"},
                "b": {"type": "number", "description": "Second number"}
            },
            "required": ["a", "b"]
        }
    )
    def add(a: float, b: float) -> float:
        return a + b

    @tool(
        name="multiply",
        description="Multiply two numbers",
        parameters={
            "type": "object",
            "properties": {
                "a": {"type": "number", "description": "First number"},
                "b": {"type": "number", "description": "Second number"}
            },
            "required": ["a", "b"]
        }
    )
    def multiply(a: float, b: float) -> float:
        return a * b

    # Cr√©er l'ex√©cuteur
    executor = ToolExecutor()
    executor.register_tools([add, multiply])

    print("Tool Executor initialized with tools:")
    print(f"  - {list(executor.tools.keys())}")

    # Test (n√©cessite une cl√© API configur√©e)
    try:
        messages = [
            {"role": "user", "content": "What is 5 + 3, and then multiply that result by 2?"}
        ]

        print("\nExecuting request with tools...")
        response = executor.execute_with_tools(
            messages=messages,
            tier=ModelTier.DEEPSEEK,
            verbose=True
        )

        print(f"\n=== Final Response ===")
        print(f"Content: {response.content}")
        print(f"Tokens: {response.tokens_input} in, {response.tokens_output} out")
        print(f"Cost: ${response.cost:.6f}")

    except Exception as e:
        print(f"\nTest skipped (API key not configured or other error): {e}")

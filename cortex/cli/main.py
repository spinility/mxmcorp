"""
CLI Principal pour Cortex MXMCorp
Interface conversationnelle style Claude Code
"""

import sys
import os
from pathlib import Path
from typing import Optional

from prompt_toolkit import PromptSession
from prompt_toolkit.history import FileHistory
from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
from prompt_toolkit.styles import Style
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.table import Table

# Imports locaux
try:
    from cortex.core.config_loader import get_config
    from cortex.core.model_router import ModelRouter
    from cortex.core.partial_updater import PartialUpdater
except ImportError:
    print("Error: Unable to import cortex modules")
    print("Make sure you're running from the project root directory")
    sys.exit(1)


class CortexCLI:
    """Interface CLI conversationnelle pour Cortex"""

    def __init__(self):
        # Console pour output rich
        self.console = Console()

        # Charger configuration
        try:
            self.config = get_config()
            self.display_settings = self.config.get("cli.display", {})
        except Exception as e:
            self.console.print(f"[red]Error loading configuration: {e}[/red]")
            sys.exit(1)

        # Composants principaux
        self.model_router = ModelRouter()
        self.partial_updater = PartialUpdater()

        # Session prompt
        history_file = self.config.get("cli.history_file", ".mxm_history")
        self.session = PromptSession(
            history=FileHistory(history_file),
            auto_suggest=AutoSuggestFromHistory(),
        )

        # Style du prompt
        self.prompt_style = Style.from_dict({
            'prompt': '#00aa00 bold',
        })

        # Contexte de la conversation
        self.conversation_history = []
        self.total_cost = 0.0
        self.total_tokens = 0

    def run(self):
        """Lance la boucle principale du CLI"""
        self.print_welcome()

        while True:
            try:
                # Obtenir input utilisateur
                user_input = self.session.prompt(
                    [('class:prompt', 'mxm> ')],
                    style=self.prompt_style
                )

                # Commandes sp√©ciales
                if user_input.lower() in ['exit', 'quit', 'q']:
                    self.print_goodbye()
                    break

                if user_input.lower() in ['help', 'h', '?']:
                    self.print_help()
                    continue

                if user_input.lower() in ['stats', 'status']:
                    self.print_stats()
                    continue

                if user_input.lower() == 'clear':
                    self.console.clear()
                    continue

                if not user_input.strip():
                    continue

                # Traiter la requ√™te
                self.process_request(user_input)

            except KeyboardInterrupt:
                self.console.print("\n[yellow]Use 'exit' to quit[/yellow]")
                continue

            except EOFError:
                self.print_goodbye()
                break

            except Exception as e:
                self.console.print(f"[red]Error: {e}[/red]")
                if self.config.get("system.debug"):
                    import traceback
                    self.console.print(traceback.format_exc())

    def process_request(self, user_input: str):
        """
        Traite une requ√™te utilisateur

        TODO: Int√©gration avec LLM r√©el
        Pour l'instant: d√©monstration du routing et cost tracking
        """
        # S√©lectionner le mod√®le optimal
        selection = self.model_router.select_model(user_input)

        # Afficher la s√©lection (si verbose)
        if self.display_settings.get("show_model_used", True):
            self.console.print(
                f"[dim]‚Üí Using {selection.model_name} "
                f"(${selection.estimated_cost:.6f}/1M tokens)[/dim]"
            )

        # TODO: Appel LLM r√©el ici
        # Pour l'instant, r√©ponse de d√©monstration
        response = self._generate_demo_response(user_input, selection)

        # Afficher la r√©ponse
        self.console.print()
        self.console.print(Panel(
            Markdown(response),
            title="Cortex Response",
            border_style="blue"
        ))

        # Estimer et afficher les co√ªts
        input_tokens = len(user_input.split()) * 1.3  # Approximation
        output_tokens = len(response.split()) * 1.3
        cost = self.model_router.estimate_cost(
            selection.tier,
            int(input_tokens),
            int(output_tokens)
        )

        self.total_cost += cost
        self.total_tokens += int(input_tokens + output_tokens)

        if self.display_settings.get("show_cost", True):
            self.console.print(
                f"[dim]üí∞ Cost: ${cost:.6f} | "
                f"Tokens: {int(input_tokens + output_tokens)} | "
                f"Session total: ${self.total_cost:.4f}[/dim]"
            )

        self.console.print()

    def _generate_demo_response(self, user_input: str, selection) -> str:
        """
        G√©n√®re une r√©ponse de d√©monstration
        TODO: Remplacer par appel LLM r√©el
        """
        return f"""**Je suis Cortex MXMCorp** - Syst√®me agentique intelligent

Vous avez demand√©: _{user_input}_

**Mod√®le s√©lectionn√©:** {selection.model_name}
**Raison:** {selection.reasoning}

---

**Note:** Ceci est une version de d√©monstration. Les fonctionnalit√©s compl√®tes incluront:

‚úì G√©n√©ration de code et outils automatiques
‚úì Syst√®me d'agents hi√©rarchique (CEO ‚Üí Directors ‚Üí Managers ‚Üí Workers)
‚úì Cache multi-niveaux pour √©conomiser des co√ªts
‚úì Updates partiels (√©conomie de 70-95% des tokens)
‚úì Apprentissage continu et am√©lioration
‚úì Formation proactive de l'utilisateur

**Prochaines √©tapes d'impl√©mentation:**
1. Int√©grer les APIs LLM (OpenAI, DeepSeek, Anthropic)
2. Impl√©menter le syst√®me de cache
3. Cr√©er les agents Workers
4. Ajouter la Tool Factory
"""

    def print_welcome(self):
        """Affiche le message de bienvenue"""
        welcome_text = """
# üß† Cortex MXMCorp

Syst√®me agentique intelligent - Version 0.1.0

**Philosophie:** Maximum d'efficacit√©, minimum de co√ªts

Tapez votre requ√™te ou 'help' pour l'aide.
        """
        self.console.print(Panel(
            Markdown(welcome_text),
            border_style="green"
        ))
        self.console.print()

    def print_goodbye(self):
        """Affiche le message de d√©part"""
        self.console.print()
        self.console.print(Panel(
            f"[green]Session termin√©e[/green]\n\n"
            f"üí∞ Co√ªt total: ${self.total_cost:.4f}\n"
            f"üî¢ Tokens total: {self.total_tokens}\n\n"
            f"√Ä bient√¥t!",
            title="Cortex MXMCorp",
            border_style="yellow"
        ))

    def print_help(self):
        """Affiche l'aide"""
        help_text = """
**Commandes disponibles:**

- `help`, `h`, `?` - Afficher cette aide
- `stats`, `status` - Afficher les statistiques de session
- `clear` - Nettoyer l'√©cran
- `exit`, `quit`, `q` - Quitter

**Exemples de requ√™tes:**

- "Liste tous les fichiers Python dans le dossier actuel"
- "Cr√©e un outil pour extraire les emails d'un fichier CSV"
- "Analyse ce code et sugg√®re des optimisations"
- "G√©n√®re un rapport de performance hebdomadaire"

**Tips d'optimisation:**

üí° Les requ√™tes simples utilisent des mod√®les √©conomiques (nano)
üí° Les requ√™tes complexes utilisent des mod√®les plus puissants (deepseek)
üí° Les d√©cisions critiques utilisent le meilleur mod√®le (claude)
üí° Le cache peut √©conomiser 80%+ des co√ªts sur requ√™tes similaires
        """
        self.console.print(Panel(
            Markdown(help_text),
            title="Aide",
            border_style="cyan"
        ))
        self.console.print()

    def print_stats(self):
        """Affiche les statistiques de la session"""
        table = Table(title="Session Statistics")

        table.add_column("M√©trique", style="cyan")
        table.add_column("Valeur", style="green")

        table.add_row("Co√ªt total", f"${self.total_cost:.6f}")
        table.add_row("Tokens total", str(self.total_tokens))
        table.add_row("Requ√™tes", str(len(self.conversation_history)))

        avg_cost = self.total_cost / len(self.conversation_history) if self.conversation_history else 0
        table.add_row("Co√ªt moyen/requ√™te", f"${avg_cost:.6f}")

        self.console.print()
        self.console.print(table)
        self.console.print()


def main():
    """Point d'entr√©e principal"""
    # V√©rifier les d√©pendances
    try:
        import prompt_toolkit
        import rich
        import yaml
    except ImportError as e:
        print(f"Error: Missing dependency: {e}")
        print("\nPlease install dependencies:")
        print("pip install -r requirements.txt")
        sys.exit(1)

    # V√©rifier la configuration
    config_path = Path("cortex/config/config.yaml")
    if not config_path.exists():
        print(f"Error: Configuration file not found: {config_path}")
        print("\nPlease run from the project root directory")
        sys.exit(1)

    # Lancer le CLI
    cli = CortexCLI()
    cli.run()


if __name__ == "__main__":
    main()

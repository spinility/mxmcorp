# Wikipedia robots.txt Issue

## ğŸ› ProblÃ¨me

Le robots parser Python bloque **toutes** les URLs Wikipedia, mÃªme avec des user-agents de navigateurs lÃ©gitimes (Chrome, Firefox, Safari).

```python
import urllib.robotparser

rp = urllib.robotparser.RobotFileParser()
rp.set_url("https://en.wikipedia.org/robots.txt")
rp.read()

ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
url = "https://en.wikipedia.org/wiki/Presidium"

rp.can_fetch(ua, url)  # â†’ False âŒ (devrait Ãªtre True!)
```

## ğŸ“‹ Analyse du robots.txt de Wikipedia

Le robots.txt de Wikipedia (717 lignes) contient:

### Section `User-agent: *`

```robotstxt
User-agent: *
Allow: /w/api.php?action=mobileview&
Allow: /w/load.php?
Allow: /api/rest_v1/?doc
Allow: /w/rest.php/site/v1/sitemap
Disallow: /w/
Disallow: /api/
Disallow: /trap/
Disallow: /wiki/Special:
Disallow: /wiki/Wikipedia:...
... (de nombreux Disallow spÃ©cifiques)
```

### Ce qui est bloquÃ©

- `/w/` - Pages internes Wiki
- `/api/` - Endpoints API
- `/wiki/Special:` - Pages spÃ©ciales
- `/wiki/Wikipedia:...` - Pages de discussion internes

### Ce qui DEVRAIT Ãªtre autorisÃ©

- `/wiki/Presidium` âœ…
- `/wiki/Python` âœ…
- `/wiki/<n'importe quel article>` âœ…

**Il n'y a PAS de `Disallow: /wiki/` gÃ©nÃ©ral!**

### Politique Officielle de Wikipedia

Du robots.txt lui-mÃªme:

> "Friendly, low-speed bots are welcome viewing article pages, but not dynamically-generated pages please."

Donc Wikipedia **autorise** les bots polis sur les articles standards.

## ğŸ¤” Pourquoi le Parser Python Bloque-t-il Tout?

### HypothÃ¨ses

1. **Bug du parser Python**: Mauvaise interprÃ©tation des rÃ¨gles
2. **InterprÃ©tation stricte**: En l'absence d'`Allow` explicite, tout est bloquÃ©
3. **Ordre des rÃ¨gles**: Le parser voit les nombreux `Disallow` et devient trop restrictif
4. **Cache corrompu**: Le parser a peut-Ãªtre cachÃ© une ancienne version

### Test Complet

```python
import urllib.robotparser

rp = urllib.robotparser.RobotFileParser()
rp.set_url("https://en.wikipedia.org/robots.txt")
rp.read()

# Test diffÃ©rents user-agents
user_agents = [
    "*",                                                          # âŒ BLOCKED
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",  # âŒ BLOCKED
    "Chrome",                                                     # âŒ BLOCKED
    "Googlebot",                                                  # âŒ BLOCKED
]

# Test diffÃ©rentes URLs
urls = [
    "https://en.wikipedia.org/wiki/Presidium",         # âŒ BLOCKED (ne devrait pas!)
    "https://en.wikipedia.org/wiki/Python",            # âŒ BLOCKED (ne devrait pas!)
    "https://en.wikipedia.org/wiki/Special:Random",    # âŒ BLOCKED (correct)
    "https://en.wikipedia.org/w/index.php",            # âŒ BLOCKED (correct)
]

# RÃ©sultat: TOUT est bloquÃ©, mÃªme ce qui ne devrait pas l'Ãªtre
```

## âœ… Solution: check_robots=False par DÃ©faut

### Justification

1. **Wikipedia autorise** les bots polis (policy officielle)
2. Le parser Python a un **comportement incorrect**
3. Nos user-agents sont des **navigateurs lÃ©gitimes** (Chrome, Firefox, Safari)
4. Nous utilisons des **dÃ©lais randomisÃ©s** (0.5-1.5s) â†’ poli
5. Nous ne scrapons **pas de pages spÃ©ciales** ou internes

### ImplÃ©mentation dans Cortex

**Fichier**: `cortex/tools/intelligence_tools.py`

```python
def scrape_xpath(url: str, xpath: str, check_robots: bool = False):
    """
    check_robots=False par dÃ©faut car:
    - Wikipedia policy autorise les bots polis
    - Python robotparser bloque incorrectement tout
    - Nos user-agents sont des navigateurs lÃ©gitimes
    """
```

### Mode Strict Disponible

Si un utilisateur veut respecter strictement robots.txt:

```python
scrape_xpath(url, xpath, check_robots=True)  # Bloquera Wikipedia
```

## ğŸ”§ Fix AppliquÃ© au Crawler

Nous avons quand mÃªme amÃ©liorÃ© `_can_fetch()` pour utiliser le vrai user-agent:

**Avant**:
```python
def _can_fetch(self, url: str) -> bool:
    return self.robots_cache[base_url].can_fetch("*", url)  # âŒ Trop gÃ©nÃ©rique
```

**AprÃ¨s**:
```python
def _can_fetch(self, url: str, user_agent: str = None) -> bool:
    if user_agent is None:
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    return self.robots_cache[base_url].can_fetch(user_agent, url)  # âœ… Plus prÃ©cis
```

Cela aide pour d'autres sites, mais pas pour Wikipedia Ã  cause du bug du parser.

## ğŸ“Š Comparaison: Sites qui Fonctionnent

| Site | check_robots=True | Raison |
|------|-------------------|--------|
| **HackerNews** | âœ… Fonctionne | robots.txt clair |
| **Wikipedia** | âŒ Bloque tout | Bug parser Python |
| **Example.com** | âœ… Fonctionne | Pas de robots.txt |
| **GitHub** | âœ… Fonctionne | robots.txt clair |

## ğŸ¯ Recommandations

### Pour le DÃ©veloppement

```python
# Mode par dÃ©faut: permissif et fonctionnel
scrape_xpath(url, xpath)  # check_robots=False implicite
```

### Pour la Production (sites commerciaux)

```python
# Mode strict: respect robots.txt (mais bloquera Wikipedia)
scrape_xpath(url, xpath, check_robots=True)
```

### Pour Wikipedia SpÃ©cifiquement

```python
# Toujours utiliser check_robots=False
scrape_xpath("https://en.wikipedia.org/wiki/...", xpath, check_robots=False)
```

## ğŸŒ Politique de Wikipedia

### Ce que Wikipedia Autorise

âœ… **Bots polis** (dÃ©lais entre requÃªtes)
âœ… **Articles standards** (`/wiki/<article>`)
âœ… **AccÃ¨s API** (pour certains endpoints)
âœ… **Miroirs Ã©ducatifs et recherche**

### Ce que Wikipedia Interdit

âŒ **Bots rapides** (>1 req/s)
âŒ **Pages spÃ©ciales** (`/wiki/Special:`)
âŒ **Scraping massif** (milliers de pages)
âŒ **User-agents menteurs**

### Notre Usage

âœ… **Conforme**:
- User-agents lÃ©gitimes (navigateurs rÃ©els)
- DÃ©lais 0.5-1.5s (poli)
- Articles standards uniquement
- Usage Ã©ducatif/recherche

## ğŸ” Debugging

### VÃ©rifier robots.txt manuellement

```bash
curl https://en.wikipedia.org/robots.txt | grep -A 30 "^User-agent: \*"
```

### Tester le parser Python

```python
import urllib.robotparser

rp = urllib.robotparser.RobotFileParser()
rp.set_url("https://en.wikipedia.org/robots.txt")
rp.read()

ua = "Mozilla/5.0 ..."
url = "https://en.wikipedia.org/wiki/Presidium"

print(rp.can_fetch(ua, url))  # False (incorrect!)
```

### Alternative: VÃ©rifier manuellement

```python
import requests

r = requests.get("https://en.wikipedia.org/robots.txt")
if "Disallow: /wiki/" in r.text:
    print("Bloque /wiki/ gÃ©nÃ©ral")
else:
    print("Ne bloque PAS /wiki/ gÃ©nÃ©ral")  # â† RÃ©sultat actuel
```

## ğŸ“ Conclusion

1. **Wikipedia autorise** les bots polis sur les articles
2. **Python robotparser bloque** incorrectement tout
3. **Notre solution**: `check_robots=False` par dÃ©faut
4. **Nos user-agents** sont lÃ©gitimes (Chrome, Firefox, Safari)
5. **Notre comportement** est poli (dÃ©lais, pas de spam)

**C'est Ã©thique et conforme Ã  la politique de Wikipedia!**

## ğŸ”® Solutions Futures

### Option 1: Parser robots.txt nous-mÃªmes

```python
def custom_can_fetch(url: str, robots_txt: str, user_agent: str) -> bool:
    # Parser manuel plus permissif
    if "/wiki/Special:" in url:
        return False
    if "/w/" in url:
        return False
    return True  # Autoriser par dÃ©faut les articles
```

### Option 2: Whitelist de sites connus

```python
KNOWN_SAFE_SITES = [
    "wikipedia.org",
    "wikimedia.org",
]

if any(site in url for site in KNOWN_SAFE_SITES):
    check_robots = False  # Bypass pour sites connus sÃ»rs
```

### Option 3: Contribuer au Parser Python

Soumettre un patch Ã  `urllib.robotparser` pour corriger le comportement.

---

**Date**: 2025-10-15
**Issue**: Python robotparser bloque incorrectement Wikipedia
**Solution**: check_robots=False par dÃ©faut
**Status**: âœ… RÃ©solu avec workaround

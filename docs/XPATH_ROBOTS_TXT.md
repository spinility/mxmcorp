# XPath Scraping et robots.txt

## üéØ Probl√®me R√©solu

Avant, `scrape_xpath` bloquait sur Wikipedia et d'autres sites √† cause de robots.txt:

```python
scrape_xpath("https://en.wikipedia.org/wiki/Presidium", "//text()")
# ‚ùå Error: "Blocked by robots.txt"
```

## ‚úÖ Solution: Param√®tre `check_robots`

Le tool `scrape_xpath` a maintenant un param√®tre optionnel `check_robots`:

```python
# Mode permissif (d√©faut): Ignore robots.txt
scrape_xpath(url, xpath, check_robots=False)  # ‚úÖ Fonctionne

# Mode strict: Respect robots.txt
scrape_xpath(url, xpath, check_robots=True)   # ‚ùå Bloqu√© si interdit
```

## üìñ Usage

### Via Cortex CLI (automatique)

L'outil est disponible automatiquement via le syst√®me de routing:

```bash
cortex> extraire le texte de https://en.wikipedia.org/wiki/Presidium xpath //text()
```

Le LLM appellera automatiquement `scrape_xpath` avec `check_robots=False` par d√©faut.

### Via Python directement

```python
from cortex.tools.intelligence_tools import scrape_xpath

# Extraire tout le texte d'une page Wikipedia
result = scrape_xpath(
    url="https://en.wikipedia.org/wiki/Presidium",
    xpath="/html/body/div[2]/div/div[3]/main/div[3]/div[3]//text()",
    check_robots=False  # Ignorer robots.txt
)

if result["success"]:
    print(f"Extracted {result['count']} elements")
    print(f"XPath version: {result['xpath_version']}")

    # Joindre le texte
    full_text = " ".join(result["data"])
    print(full_text)
else:
    print(f"Error: {result['error']}")
```

### XPath 2.0 avec string-join()

```python
# XPath 2.0: Retourne directement un string au lieu d'une liste
result = scrape_xpath(
    url="https://en.wikipedia.org/wiki/Presidium",
    xpath="string-join(//div[@id='mw-content-text']//p//text(), ' ')",
    check_robots=False
)

# result["data"] = [<texte concaten√© en un seul √©l√©ment>]
```

## üîí robots.txt et √âthique

### Quand utiliser `check_robots=True`

**Mode strict recommand√© pour**:
- Scraping automatis√© et r√©current
- Sites commerciaux (respect de leurs r√®gles)
- Projets publics ou open-source
- Conformit√© l√©gale stricte

```python
scrape_xpath(url, xpath, check_robots=True)
```

### Quand utiliser `check_robots=False`

**Mode permissif acceptable pour**:
- Recherche acad√©mique et √©ducation
- Extraction ponctuelle et manuelle
- Sites avec robots.txt trop restrictifs mais contenu public
- Debugging et tests de d√©veloppement

```python
scrape_xpath(url, xpath, check_robots=False)  # D√©faut
```

### Note L√©gale

Le param√®tre `check_robots=False` **n'ignore que robots.txt**, pas les lois:
- ‚úÖ L√©gal: Scraper du contenu public pour usage personnel/recherche
- ‚ùå Ill√©gal: Violer CFAA, DMCA, RGPD, copier du contenu prot√©g√©

**Cortex est responsable**: L'outil utilise des techniques stealth (user-agent rotation, delays) mais **respecte la loi**.

## üìä Comparaison des Modes

| Aspect | `check_robots=False` | `check_robots=True` |
|--------|---------------------|---------------------|
| **Wikipedia** | ‚úÖ Fonctionne | ‚ùå Bloqu√© |
| **HackerNews** | ‚úÖ Fonctionne | ‚úÖ Fonctionne |
| **Forbes** | ‚ö†Ô∏è HTML vide (JS) | ‚ö†Ô∏è HTML vide (JS) |
| **Sites commerciaux** | ‚úÖ Fonctionne | D√©pend du robots.txt |
| **√âthique** | ‚ö†Ô∏è Zone grise | ‚úÖ Respectueux |
| **L√©galit√©** | ‚úÖ L√©gal (usage perso) | ‚úÖ L√©gal |
| **Production** | ‚ö†Ô∏è √Ä √©viter | ‚úÖ Recommand√© |

## üõ†Ô∏è Impl√©mentation Technique

### Architecture

```
scrape_xpath(url, xpath, check_robots=False)
  ‚îÇ
  ‚îú‚îÄ> StealthWebCrawler()
  ‚îÇ     ‚îú‚îÄ> User-agent rotation
  ‚îÇ     ‚îú‚îÄ> Random delays (0.5-1.5s)
  ‚îÇ     ‚îî‚îÄ> Realistic headers
  ‚îÇ
  ‚îú‚îÄ> validate_xpath(source, check_robots=False)
  ‚îÇ     ‚îú‚îÄ> Fetch HTML
  ‚îÇ     ‚îú‚îÄ> Parse with lxml
  ‚îÇ     ‚îî‚îÄ> Test XPath expression
  ‚îÇ
  ‚îî‚îÄ> scrape(source, validate_first=False)
        ‚îú‚îÄ> Extract data via XPath
        ‚îú‚îÄ> Save to cortex/data/scraped_data/
        ‚îî‚îÄ> Return ScrapedData
```

### Flux de Contr√¥le

```python
if check_robots:
    # Mode strict: Validation compl√®te + robots.txt
    result = crawler.scrape(source, validate_first=True)
    # validate_xpath() v√©rifiera robots.txt
else:
    # Mode permissif: Validation sans robots.txt
    validation = crawler.validate_xpath(source, check_robots=False)

    if not validation.success:
        return error

    # Scrape directement
    result = crawler.scrape(source, validate_first=False)
```

### M√©thode `validate_xpath`

```python
def validate_xpath(
    self,
    source: XPathSource,
    check_robots: bool = True  # Param√®tre ajout√©
) -> ValidationResult:

    # V√©rifier robots.txt seulement si demand√©
    if check_robots and not self._can_fetch(source.url):
        return ValidationResult(
            success=False,
            error="Blocked by robots.txt",
            ...
        )

    # Fetch et test XPath
    ...
```

## üìù Exemples d'Utilisation

### Exemple 1: Wikipedia (robots.txt bloque)

```python
from cortex.tools.intelligence_tools import scrape_xpath

# Mode permissif (fonctionne)
result = scrape_xpath(
    url="https://en.wikipedia.org/wiki/Python_(programming_language)",
    xpath="//h1[@id='firstHeading']/text()",
    check_robots=False
)

print(result["data"])  # ["Python (programming language)"]
```

### Exemple 2: HackerNews (robots.txt autorise)

```python
# Mode strict (fonctionne car autoris√©)
result = scrape_xpath(
    url="https://news.ycombinator.com",
    xpath="//span[@class='titleline']/a/text()",
    check_robots=True  # Autoris√© par HN
)

print(f"Top {result['count']} stories:")
for title in result["data"][:10]:
    print(f"  - {title}")
```

### Exemple 3: XPath 2.0 avec string-join()

```python
# Extraire et concat√©ner avec XPath 2.0
result = scrape_xpath(
    url="https://en.wikipedia.org/wiki/Presidium",
    xpath="string-join(//div[@id='mw-content-text']//p[position() <= 3]//text(), ' ')",
    check_robots=False
)

# R√©sultat: Un seul string avec les 3 premiers paragraphes
intro = result["data"][0]
print(f"Introduction: {intro[:500]}...")
```

### Exemple 4: Gestion d'erreurs

```python
result = scrape_xpath(url, xpath, check_robots=False)

if result["success"]:
    print(f"‚úÖ Success: {result['count']} elements")
    print(f"XPath {result['xpath_version']}")

    # Traiter les donn√©es
    for item in result["data"]:
        process(item)
else:
    print(f"‚ùå Error: {result['error']}")
    print(f"Message: {result['message']}")

    # Fallback ou retry
    if "Blocked by robots.txt" in result["error"]:
        # Essayer avec check_robots=False
        result = scrape_xpath(url, xpath, check_robots=False)
```

## üîç Debugging

### V√©rifier si un site bloque via robots.txt

```python
from cortex.departments.intelligence import StealthWebCrawler

crawler = StealthWebCrawler()

# Test avec robots.txt
if crawler._can_fetch("https://en.wikipedia.org/wiki/Presidium"):
    print("‚úÖ Autoris√© par robots.txt")
else:
    print("‚ùå Bloqu√© par robots.txt")
```

### Tester XPath sans robots.txt

```python
from cortex.departments.intelligence import StealthWebCrawler, XPathSource
from datetime import datetime

crawler = StealthWebCrawler()

source = XPathSource(
    id="test",
    name="Test",
    url="https://en.wikipedia.org/wiki/Presidium",
    xpath="//h1/text()",
    description="Test",
    category="test",
    refresh_interval_hours=0,
    created_at=datetime.now(),
    last_validated=None,
    validation_status="pending",
    last_error=None,
    enabled=True
)

# Test sans robots.txt
result = crawler.validate_xpath(source, check_robots=False)

print(f"Success: {result.success}")
print(f"Elements: {result.elements_found}")
print(f"Sample: {result.sample_data}")
```

## üìà Performance

### Avec check_robots=True

```
Request 1: Fetch robots.txt        ~200ms
Request 2: Fetch HTML page          ~800ms
Parse HTML                          ~50ms
Extract XPath                       ~10ms
-----------------------------------------
Total:                              ~1060ms
```

### Avec check_robots=False

```
Request 1: Fetch HTML page          ~800ms
Parse HTML                          ~50ms
Extract XPath                       ~10ms
-----------------------------------------
Total:                              ~860ms
```

**Gain**: ~200ms par requ√™te (20% plus rapide)

## üéØ Recommandations

### Pour le D√©veloppement

```python
# Utilisez le mode permissif pour plus de flexibilit√©
scrape_xpath(url, xpath, check_robots=False)
```

### Pour la Production

```python
# Utilisez le mode strict pour respecter les r√®gles
scrape_xpath(url, xpath, check_robots=True)
```

### Pour l'√âducation / Recherche

```python
# Mode permissif acceptable si usage ponctuel et √©thique
scrape_xpath(url, xpath, check_robots=False)
```

## üîÑ Historique des Modifications

### v1.1.0 - 2025-10-15

**Ajout du param√®tre `check_robots`**:
- `scrape_xpath()` accepte maintenant `check_robots: bool = False`
- Par d√©faut: `False` pour compatibilit√© et flexibilit√©
- Mode strict disponible avec `True`
- XPath 2.0 support√© avec `elementpath`

**Avant**:
```python
scrape_xpath(url, xpath)  # Bloqu√© par robots.txt
```

**Apr√®s**:
```python
scrape_xpath(url, xpath)  # Fonctionne (check_robots=False par d√©faut)
scrape_xpath(url, xpath, check_robots=True)  # Mode strict si n√©cessaire
```

---

**Derni√®re mise √† jour**: 2025-10-15
**Version**: 1.1.0
**Auteur**: Cortex MXMCorp
